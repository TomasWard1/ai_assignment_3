{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms for Training RNNs\n",
    "\n",
    "In this section, we will review some of the most popular optimization algorithms used to train Recurrent Neural Networks (RNNs). These include:\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD)**\n",
    "2. **SGD with Momentum**\n",
    "3. **RMSprop**\n",
    "4. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "Each algorithm has a different approach to adjusting the weights of the model to minimize the loss function. We will discuss the purpose, mechanism, advantages, and disadvantages of each.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Purpose and Mechanism:\n",
    "- **SGD** is one of the simplest and most widely used optimization algorithms.\n",
    "- It computes the gradient of the loss function with respect to the model parameters (weights) using a single sample from the training data (hence \"stochastic\").\n",
    "- The weight updates are made by subtracting the gradient scaled by a learning rate from the current weight.\n",
    "\n",
    "### Advantages:\n",
    "- Simple and easy to implement.\n",
    "- Can handle very large datasets.\n",
    "- Effective for simple convex loss functions.\n",
    "\n",
    "### Disadvantages:\n",
    "- Can be slow to converge due to noisy updates.\n",
    "- Sensitive to the choice of learning rate.\n",
    "- May struggle with complex loss functions or local minima.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. SGD with Momentum\n",
    "\n",
    "### Purpose and Mechanism:\n",
    "- **SGD with Momentum** modifies standard SGD by adding a \"momentum\" term that helps the optimizer build up speed in the relevant direction, and dampens oscillations.\n",
    "- This momentum term accumulates the gradient of previous steps to adjust the weight updates.\n",
    "\n",
    "### Advantages:\n",
    "- Faster convergence compared to standard SGD.\n",
    "- Helps escape local minima by adding momentum to the updates.\n",
    "- Stabilizes the optimization process by reducing oscillations.\n",
    "\n",
    "### Disadvantages:\n",
    "- Requires careful tuning of the momentum parameter.\n",
    "- Still sensitive to the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. RMSprop\n",
    "\n",
    "### Purpose and Mechanism:\n",
    "- **RMSprop** is an adaptive learning rate method, which divides the learning rate by a moving average of the squared gradient.\n",
    "- It addresses the issue of oscillations and slow convergence in deep networks by adjusting the learning rate for each parameter individually.\n",
    "\n",
    "### Advantages:\n",
    "- Adapts the learning rate for each parameter.\n",
    "- Good for training models with non-stationary objectives (e.g., RNNs).\n",
    "- Helps to avoid the vanishing gradient problem by stabilizing the learning process.\n",
    "\n",
    "### Disadvantages:\n",
    "- The learning rate still needs tuning.\n",
    "- May require more computational resources compared to SGD.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "### Purpose and Mechanism:\n",
    "- **Adam** is an adaptive optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "- It combines the advantages of **Momentum** and **RMSprop** by using both the momentum term and the squared gradient term.\n",
    "\n",
    "### Advantages:\n",
    "- Combines the best features of **Momentum** and **RMSprop**.\n",
    "- Adapts learning rates based on the moments of the gradients.\n",
    "- Works well for a wide range of architectures and datasets.\n",
    "- Generally requires less tuning of learning rate and other hyperparameters.\n",
    "\n",
    "### Disadvantages:\n",
    "- More computationally expensive than SGD.\n",
    "- May not generalize well in some cases, and may overfit if not tuned properly.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Differences\n",
    "\n",
    "| Algorithm                     | Purpose                                        | Advantages                                         | Disadvantages                                     |\n",
    "|-------------------------------|------------------------------------------------|---------------------------------------------------|--------------------------------------------------|\n",
    "| **SGD**                        | Standard gradient descent on individual samples | Simple, good for convex problems                  | Slow convergence, sensitive to learning rate     |\n",
    "| **SGD with Momentum**          | Adds velocity to accelerate updates            | Faster convergence, avoids local minima           | Requires tuning of momentum parameter            |\n",
    "| **RMSprop**                    | Adapts learning rate based on gradient magnitudes | Adapts learning rate for each parameter           | Still needs tuning of learning rate             |\n",
    "| **Adam**                       | Combines Momentum and RMSprop features         | Adaptive, faster convergence, less tuning needed  | More computationally expensive                   |\n",
    "\n",
    "---\n",
    "\n",
    "These optimization algorithms are fundamental in training RNNs, and each has its strengths and weaknesses depending on the specific use case and data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
